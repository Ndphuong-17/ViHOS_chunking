{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"_KFIOHagGKbc"},"outputs":[],"source":["import pandas as pd\n","from ast import literal_eval"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1641556649094,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"BSYQ85ybG7Hg","outputId":"e65ce208-1ccf-41ef-8e0b-d8b2a588b3c2"},"outputs":[],"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', None)\n","pd.options.display.max_rows"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["D:\\Project\\Toolkit_for_Preprocessing_MXH\\ViHOS_chunking\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\ndp17\\.pyenv\\pyenv-win\\versions\\3.10.5\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n","  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"]}],"source":["cd D:\\Project\\Toolkit_for_Preprocessing_MXH\\ViHOS_chunking"]},{"cell_type":"markdown","metadata":{},"source":["# Warm up"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["train_path = r\"Data\\Raw_data\\train.csv\"\n","dev_path = r\"Data\\Raw_data\\dev.csv\"\n","test_path = r\"Data\\Raw_data\\test.csv\""]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"elapsed":341,"status":"ok","timestamp":1641570592726,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"fn-nOSPGHMGw","outputId":"e51b26de-f1e2-43c7-dce2-05f35f416294"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>content</th>\n","      <th>index_spans</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Anh bar .</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Hello th·∫ßy</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0     content index_spans\n","0           0   Anh bar .          []\n","1           1  Hello th·∫ßy          []"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["train = pd.read_csv(train_path)\n","dev = pd.read_csv(dev_path)\n","test = pd.read_csv(test_path)\n","test['index_spans'] = test['index_spans'].apply(literal_eval)\n","train['index_spans'] = train['index_spans'].apply(literal_eval)\n","dev['index_spans'] = dev['index_spans'].apply(literal_eval)\n","\n","headers = ['Unnamed: 0',  'content', 'index_spans']\n","train.columns = headers\n","dev.columns = headers\n","test.columns = headers\n","test.head(2)"]},{"cell_type":"markdown","metadata":{"id":"7t9vZ6idK66i"},"source":["# Pre-processing"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["from Code.Preprocessing import unicode\n","# Apply the replacement function to the 'content' column\n","test['content'] = test['content'].apply(unicode)\n","train['content'] = train['content'].apply(unicode)\n","dev['content'] = dev['content'].apply(unicode)\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["import os\n","folder_path = \"spans_text\"\n","if not os.path.exists(folder_path):\n","    # Create the folder if it doesn't exist\n","    os.makedirs(folder_path)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["df = train[['index_spans', 'content']]\n","df.columns = ['spans', 'text']\n","df.to_csv(r'spans_text\\df_train.csv')\n","\n","df = dev[['index_spans', 'content']]\n","df.columns = ['spans', 'text']\n","df.to_csv(r'spans_text\\df_dev.csv')\n","\n","df = test[['index_spans', 'content']]\n","df.columns = ['spans', 'text']\n","df.to_csv(r'spans_text\\df_test.csv')"]},{"cell_type":"markdown","metadata":{"id":"VvaXLDO6PIcA"},"source":["# Tokeniner"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"pnL6rnlKHsik"},"outputs":[],"source":["# from vncorenlp import VnCoreNLP\n","# annotator = VnCoreNLP(r\"D:\\Project\\Toolkit_for_Preprocessing_MXH\\ViHOS\\Code\\VnCoreNLP\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n","\n","from vncorenlp import VnCoreNLP\n","\n","# Ensure that the JAR file path is correct\n","annotator = VnCoreNLP(r\"D:\\\\Project\\\\Toolkit_for_Preprocessing_MXH\\\\ViHOS\\\\Code\\\\VnCoreNLP\\\\VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1641180328015,"user":{"displayName":"L∆∞u ƒê·ª©c C·∫£nh","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"15467637707317635561"},"user_tz":-420},"id":"DtPB0PVULquy","outputId":"899752fe-72fb-41b1-9474-9a71e689d496"},"outputs":[{"data":{"text/plain":["['A', 'm√©o', 'n√™n', 'v·ªÅ', 'n∆∞·ªõc', 'th√¨', 'm·ªõi', 'ƒë√∫ng']"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["text = test['content'][325]\n","annotator_text = annotator.tokenize(text)\n","tokens = []\n","for i in range(len(annotator_text)):\n","  for j in range(len(annotator_text[i])):\n","    tokens.append(annotator_text[i][j])\n","tokens\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def tokenize_word(text, pos):\n","    tokens = [token for sentence in annotator.tokenize(text) for token in sentence]\n","    alignment, start = [], 0\n","\n","    for t in tokens:\n","        if t == \"_\":\n","            res = text.find(t, start)\n","        else:\n","            t = t.lstrip(\"_\").replace(\"_\", \" \")\n","            res = text.find(t, start)\n","\n","        alignment.append(pos[res:res + len(t)])\n","        start = res + len(t)\n","\n","    assert len(tokens) == len(alignment)\n","    return tokens, alignment\n","\n","def annotate(spans, alignment, tokens):\n","    annotations = pd.DataFrame({'Tokens': tokens, 'Tag': ['O'] * len(tokens)})\n","\n","    for span in spans:\n","        for i, align in enumerate(alignment):\n","            if align[-1] < span[0]:\n","                continue\n","            elif align[0] <= span[0] <= align[-1]:\n","                annotations.at[i, 'Tag'] = 'B-T'\n","            elif span[0] < align[0] <= span[-1]:\n","                annotations.at[i, 'Tag'] = 'I-T'\n","            elif align[0] > span[-1]:\n","                break\n","\n","    return annotations['Tag']\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from Code.Preprocessing import load_data"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"Fia3UwtPpNxc"},"outputs":[{"data":{"text/plain":["{'text': 'ƒë m, th·∫ßy gi√°o c≈©ng s·ªëng ·∫£o',\n"," 'spans': [[0, 2]],\n"," 'text_spans': ['ƒë m']}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["test_data = load_data(r'spans_text\\df_test.csv')\n","train_data = load_data(r'spans_text\\df_train.csv')\n","dev_data = load_data(r'spans_text\\df_dev.csv')\n","test_data[30]"]},{"cell_type":"markdown","metadata":{},"source":["# Annotate sentence-based Chunking"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["train_path = (\"Data\\spans_text\\df_train.csv\")\n","dev_path = (\"Data\\spans_text\\df_dev.csv\")\n","test_path = (\"Data\\spans_text\\df_test.csv\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["{'text': 'Kh√¥ng th·ªÉ t∆∞·ªüng t∆∞·ª£ng n·ªïi... B·ªçn ch√∫ng ƒÉn b·∫•t ch·∫•p th·ªß ƒëo·∫°n',\n"," 'spans': [[29, 37], [42, 58]],\n"," 'text_spans': ['B·ªçn ch√∫ng', 'b·∫•t ch·∫•p th·ªß ƒëo·∫°n']}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["from Code.Preprocessing import load_data\n","train_data = load_data(train_path)\n","dev_data = load_data(dev_path)\n","test_data = load_data(test_path)\n","test_data[128]"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["from Code.Preprocessing import dupplicate_punctuation, sentence_based_chunking\n","import numpy as np\n","import pandas as pd\n","\n","def data_chunking(data):\n","    formated_data = []\n","\n","\n","    for d in data:\n","        text, spans, text_spans = d['text'], d['spans'], d['text_spans']\n","        pos = list(range(len(text)))  # Create a position list\n","        text, pos, spans = dupplicate_punctuation(text, pos, spans)  # Clean up punctuation\n","        annotations = sentence_based_chunking(text, pos, spans)\n","        annotations = [annotation.values() for annotation in annotations]\n","\n","        # Combine tokens and their corresponding annotations\n","        formated_data.extend(annotations)  # Using zip for better performance\n","        formated_data.append((None, None, None))  # Append a marker for sentence separation\n","\n","\n","    # Create a DataFrame from the formatted data\n","    df_final = pd.DataFrame(formated_data, columns=['Chunk', 'Tag', 'Spans'])\n","\n","    # Generate sentence IDs\n","    sentence_id = []\n","    sentence = 0\n","    for word in df_final['Chunk']:\n","        if word is not None:\n","            sentence_id.append(sentence)\n","        else:\n","            sentence_id.append(np.nan)\n","            sentence += 1\n","\n","    df_final['sentence_id'] = sentence_id\n","    df_final.dropna(inplace=True)  # Remove rows where Word is None\n","    df_final['sentence_id'] = df_final['sentence_id'].astype(\"int64\")  # Convert to int64\n","    df_final = df_final[['Chunk', 'Tag', 'sentence_id', 'Spans']]\n","\n","    return df_final"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":21204,"status":"ok","timestamp":1641570640444,"user":{"displayName":"Nap Lan Dau","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgyLJ7xblBnvLEyFishN4Q4xZSVXTxeaLzYRJU=s64","userId":"01595996840706054717"},"user_tz":-420},"id":"aHoIEsFIrAt_","outputId":"42c7b9f6-57ce-4868-8b65-362a17d88da5"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Chunk</th>\n","      <th>Tag</th>\n","      <th>sentence_id</th>\n","      <th>Spans</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Anh bar .</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Hello th·∫ßy</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>T·ª´ l√∫c m·∫•y bro cmt c·ª±c k√¨ cl g√¨ ƒë·∫•y l√† r·∫•t kh√≥ ƒë·ªÉ ng√≥ ƒë∆∞·ª£c c√°i cmt hay bu·ªìn th·ª±c s·ª± üôÅ</td>\n","      <td>1.0</td>\n","      <td>2</td>\n","      <td>[[26, 27]]</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Nghe tr·∫ßn d·∫ßn th·∫•y c∆∞·ªùi ƒëau c·∫£ b·ª•ng</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Idol tui c√≥ kh√°c.</td>\n","      <td>0.0</td>\n","      <td>4</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                   Chunk  \\\n","0                                                                              Anh bar .   \n","2                                                                             Hello th·∫ßy   \n","4  T·ª´ l√∫c m·∫•y bro cmt c·ª±c k√¨ cl g√¨ ƒë·∫•y l√† r·∫•t kh√≥ ƒë·ªÉ ng√≥ ƒë∆∞·ª£c c√°i cmt hay bu·ªìn th·ª±c s·ª± üôÅ   \n","6                                                    Nghe tr·∫ßn d·∫ßn th·∫•y c∆∞·ªùi ƒëau c·∫£ b·ª•ng   \n","8                                                                      Idol tui c√≥ kh√°c.   \n","\n","   Tag  sentence_id       Spans  \n","0  0.0            0          []  \n","2  0.0            1          []  \n","4  1.0            2  [[26, 27]]  \n","6  0.0            3          []  \n","8  0.0            4          []  "]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["test = data_chunking(test_data)\n","train = data_chunking(train_data)\n","dev = data_chunking(dev_data)\n","test.head()"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["train.reset_index(inplace=True)\n","dev.reset_index(inplace=True)\n","test.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["import os\n","folder_path = \"Data\\Chunking_data\"\n","if not os.path.exists(folder_path):\n","    # Create the folder if it doesn't exist\n","    os.makedirs(folder_path)\n","\n","train.to_csv(r'Data\\Chunking_data\\train.csv', index=False)\n","dev.to_csv(r'Data\\Chunking_data\\dev.csv', index=False)\n","test.to_csv(r'Data\\Chunking_data\\test.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
